{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.tag import pos_tag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "ex = \"I am a medical professional running a group of hospitals in India with clients all around the world. As a business venture we a team of 670 doctors have decided to come together on an digital platform where the clients can interact with doctors based on their need and auto fix appointment to the nearest branch without the need of directly contacting the branch. I expect this to be solved by hgs.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(sent):\n",
    "    sent = nltk.word_tokenize(sent)\n",
    "    sent = nltk.pos_tag(sent)\n",
    "    return sent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "sent = preprocess(ex)\n",
    "#sent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  noun phrase chunking to identify named entities using a \n",
    "# regular expression consisting of rules that indicate how sentences should be chunked\n",
    "pattern = 'NP: {<DT>?<JJ>*<NN>}'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(S\n",
      "  I/PRP\n",
      "  am/VBP\n",
      "  (NP a/DT medical/JJ professional/NN)\n",
      "  running/VBG\n",
      "  (NP a/DT group/NN)\n",
      "  of/IN\n",
      "  hospitals/NNS\n",
      "  in/IN\n",
      "  India/NNP\n",
      "  with/IN\n",
      "  clients/NNS\n",
      "  all/DT\n",
      "  around/IN\n",
      "  (NP the/DT world/NN)\n",
      "  ./.\n",
      "  As/IN\n",
      "  (NP a/DT business/NN)\n",
      "  (NP venture/NN)\n",
      "  we/PRP\n",
      "  (NP a/DT team/NN)\n",
      "  of/IN\n",
      "  670/CD\n",
      "  doctors/NNS\n",
      "  have/VBP\n",
      "  decided/VBN\n",
      "  to/TO\n",
      "  come/VB\n",
      "  together/RB\n",
      "  on/IN\n",
      "  (NP an/DT digital/JJ platform/NN)\n",
      "  where/WRB\n",
      "  the/DT\n",
      "  clients/NNS\n",
      "  can/MD\n",
      "  interact/VB\n",
      "  with/IN\n",
      "  doctors/NNS\n",
      "  based/VBN\n",
      "  on/IN\n",
      "  their/PRP$\n",
      "  (NP need/NN)\n",
      "  and/CC\n",
      "  (NP auto/NN)\n",
      "  fix/VBP\n",
      "  (NP appointment/NN)\n",
      "  to/TO\n",
      "  the/DT\n",
      "  nearest/JJS\n",
      "  (NP branch/NN)\n",
      "  without/IN\n",
      "  (NP the/DT need/NN)\n",
      "  of/IN\n",
      "  directly/RB\n",
      "  contacting/VBG\n",
      "  (NP the/DT branch/NN)\n",
      "  ./.\n",
      "  I/PRP\n",
      "  expect/VBP\n",
      "  this/DT\n",
      "  to/TO\n",
      "  be/VB\n",
      "  solved/VBN\n",
      "  by/IN\n",
      "  (NP hgs/NN)\n",
      "  ./.)\n"
     ]
    }
   ],
   "source": [
    "cp = nltk.RegexpParser(pattern)\n",
    "cs = cp.parse(sent)\n",
    "print(cs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('I', 'PRP', 'O'),\n",
      " ('am', 'VBP', 'O'),\n",
      " ('a', 'DT', 'B-NP'),\n",
      " ('medical', 'JJ', 'I-NP'),\n",
      " ('professional', 'NN', 'I-NP'),\n",
      " ('running', 'VBG', 'O'),\n",
      " ('a', 'DT', 'B-NP'),\n",
      " ('group', 'NN', 'I-NP'),\n",
      " ('of', 'IN', 'O'),\n",
      " ('hospitals', 'NNS', 'O'),\n",
      " ('in', 'IN', 'O'),\n",
      " ('India', 'NNP', 'O'),\n",
      " ('with', 'IN', 'O'),\n",
      " ('clients', 'NNS', 'O'),\n",
      " ('all', 'DT', 'O'),\n",
      " ('around', 'IN', 'O'),\n",
      " ('the', 'DT', 'B-NP'),\n",
      " ('world', 'NN', 'I-NP'),\n",
      " ('.', '.', 'O'),\n",
      " ('As', 'IN', 'O'),\n",
      " ('a', 'DT', 'B-NP'),\n",
      " ('business', 'NN', 'I-NP'),\n",
      " ('venture', 'NN', 'B-NP'),\n",
      " ('we', 'PRP', 'O'),\n",
      " ('a', 'DT', 'B-NP'),\n",
      " ('team', 'NN', 'I-NP'),\n",
      " ('of', 'IN', 'O'),\n",
      " ('670', 'CD', 'O'),\n",
      " ('doctors', 'NNS', 'O'),\n",
      " ('have', 'VBP', 'O'),\n",
      " ('decided', 'VBN', 'O'),\n",
      " ('to', 'TO', 'O'),\n",
      " ('come', 'VB', 'O'),\n",
      " ('together', 'RB', 'O'),\n",
      " ('on', 'IN', 'O'),\n",
      " ('an', 'DT', 'B-NP'),\n",
      " ('digital', 'JJ', 'I-NP'),\n",
      " ('platform', 'NN', 'I-NP'),\n",
      " ('where', 'WRB', 'O'),\n",
      " ('the', 'DT', 'O'),\n",
      " ('clients', 'NNS', 'O'),\n",
      " ('can', 'MD', 'O'),\n",
      " ('interact', 'VB', 'O'),\n",
      " ('with', 'IN', 'O'),\n",
      " ('doctors', 'NNS', 'O'),\n",
      " ('based', 'VBN', 'O'),\n",
      " ('on', 'IN', 'O'),\n",
      " ('their', 'PRP$', 'O'),\n",
      " ('need', 'NN', 'B-NP'),\n",
      " ('and', 'CC', 'O'),\n",
      " ('auto', 'NN', 'B-NP'),\n",
      " ('fix', 'VBP', 'O'),\n",
      " ('appointment', 'NN', 'B-NP'),\n",
      " ('to', 'TO', 'O'),\n",
      " ('the', 'DT', 'O'),\n",
      " ('nearest', 'JJS', 'O'),\n",
      " ('branch', 'NN', 'B-NP'),\n",
      " ('without', 'IN', 'O'),\n",
      " ('the', 'DT', 'B-NP'),\n",
      " ('need', 'NN', 'I-NP'),\n",
      " ('of', 'IN', 'O'),\n",
      " ('directly', 'RB', 'O'),\n",
      " ('contacting', 'VBG', 'O'),\n",
      " ('the', 'DT', 'B-NP'),\n",
      " ('branch', 'NN', 'I-NP'),\n",
      " ('.', '.', 'O'),\n",
      " ('I', 'PRP', 'O'),\n",
      " ('expect', 'VBP', 'O'),\n",
      " ('this', 'DT', 'O'),\n",
      " ('to', 'TO', 'O'),\n",
      " ('be', 'VB', 'O'),\n",
      " ('solved', 'VBN', 'O'),\n",
      " ('by', 'IN', 'O'),\n",
      " ('hgs', 'NN', 'B-NP'),\n",
      " ('.', '.', 'O')]\n"
     ]
    }
   ],
   "source": [
    "from nltk.chunk import conlltags2tree, tree2conlltags\n",
    "from pprint import pprint\n",
    "iob_tagged = tree2conlltags(cs)\n",
    "pprint(iob_tagged)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(S\n",
      "  I/PRP\n",
      "  am/VBP\n",
      "  a/DT\n",
      "  medical/JJ\n",
      "  professional/NN\n",
      "  running/VBG\n",
      "  a/DT\n",
      "  group/NN\n",
      "  of/IN\n",
      "  hospitals/NNS\n",
      "  in/IN\n",
      "  (GPE India/NNP)\n",
      "  with/IN\n",
      "  clients/NNS\n",
      "  all/DT\n",
      "  around/IN\n",
      "  the/DT\n",
      "  world/NN\n",
      "  ./.\n",
      "  As/IN\n",
      "  a/DT\n",
      "  business/NN\n",
      "  venture/NN\n",
      "  we/PRP\n",
      "  a/DT\n",
      "  team/NN\n",
      "  of/IN\n",
      "  670/CD\n",
      "  doctors/NNS\n",
      "  have/VBP\n",
      "  decided/VBN\n",
      "  to/TO\n",
      "  come/VB\n",
      "  together/RB\n",
      "  on/IN\n",
      "  an/DT\n",
      "  digital/JJ\n",
      "  platform/NN\n",
      "  where/WRB\n",
      "  the/DT\n",
      "  clients/NNS\n",
      "  can/MD\n",
      "  interact/VB\n",
      "  with/IN\n",
      "  doctors/NNS\n",
      "  based/VBN\n",
      "  on/IN\n",
      "  their/PRP$\n",
      "  need/NN\n",
      "  and/CC\n",
      "  auto/NN\n",
      "  fix/VBP\n",
      "  appointment/NN\n",
      "  to/TO\n",
      "  the/DT\n",
      "  nearest/JJS\n",
      "  branch/NN\n",
      "  without/IN\n",
      "  the/DT\n",
      "  need/NN\n",
      "  of/IN\n",
      "  directly/RB\n",
      "  contacting/VBG\n",
      "  the/DT\n",
      "  branch/NN\n",
      "  ./.\n",
      "  I/PRP\n",
      "  expect/VBP\n",
      "  this/DT\n",
      "  to/TO\n",
      "  be/VB\n",
      "  solved/VBN\n",
      "  by/IN\n",
      "  hgs/NN\n",
      "  ./.)\n"
     ]
    }
   ],
   "source": [
    "ne_tree = nltk.ne_chunk(pos_tag(word_tokenize(ex)))\n",
    "print(ne_tree)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "from spacy.lang.en.examples import sentences\n",
    "\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "doc = nlp(sentences[0])\n",
    "print(doc.text)\n",
    "for token in doc:\n",
    "    print(token.text, token.pos_, token.dep_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "from spacy import displacy\n",
    "from collections import Counter\n",
    "import en_core_web_sm\n",
    "nlp = en_core_web_sm.load()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ram PERSON\n",
      "India GPE\n"
     ]
    }
   ],
   "source": [
    "#doc = nlp('I am a medical professional running a group of hospitals in India with clients all around the world. As a business venture we a team of 670 doctors have decided to come together on an digital platform where the clients can interact with doctors based on their need and auto fix appointment to the nearest branch without the need of directly contacting the branch. I expect this to be solved by hgs.')\n",
    "doc = nlp('I am the managing director of the Ram group of restaurants in India, I would like to digitalize my business by integrating all the braches under a single database which includes the bill details, stock purchase,etc. of all the branches. So I expect an optimum solution from hgs.')\n",
    "for ent in doc.ents:\n",
    "    print(ent.text, ent.label_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(NE India/NNP)\n"
     ]
    }
   ],
   "source": [
    "# Tokenize the article into sentences: sentences\n",
    "sentences = nltk.sent_tokenize(ex)\n",
    "\n",
    "# Tokenize each sentence into words: token_sentences\n",
    "token_sentences = [word_tokenize(sent) for sent in sentences]\n",
    "\n",
    "# Tag each tokenized sentence into parts of speech: pos_sentences\n",
    "pos_sentences = [nltk.pos_tag(sent) for sent in token_sentences] \n",
    "\n",
    "# Create the named entity chunks: chunked_sentences\n",
    "chunked_sentences = nltk.ne_chunk_sents(pos_sentences, binary = True)\n",
    "\n",
    "# Test for stems of the tree with 'NE' tags\n",
    "for sent in chunked_sentences:\n",
    "    for chunk in sent:\n",
    "        if hasattr(chunk, \"label\") and chunk.label() == \"NE\":\n",
    "            print(chunk)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "items = [x.text for x in doc.ents]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['India', '670']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "items"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
